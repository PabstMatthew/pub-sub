#lang ivy1.8

# STL
include numbers
include network
include order
include collections
include timeout

# Project-specific
include shard
include table
include delmap
include multi_paxos

# TODO: timeout mechanism, publishes
include timeout

# Types.
global {
    # IDs for different processes.
    instance client_id : iterable
    instance server_id : iterable
    instance manager_id : iterable

    # Key type used to identify pub/sub channels.
    instance key_t : iterable

    # Value types used in channels.
    alias byte = uint[8]
    alias data_t = byte
    class val_t = {
        field data : data_t
        field src : client_id
    }
    instance log_t : vector(val_t)  # a history of channel messages
    instance client_list : vector(client_id)
    #instance clientset(C:client_id)

    # Channel state.
    class channel_t = {
        field log : log_t # channel message history
        #field client_subscribed(C:client_id) : bool # set of clients subscribed to this channel
        field clients_subscribed : client_list # list of clients subscribed to this channel
    }

    # Specification types.
    instance pub_id : iterable
    instance commit_id : iterable
}

process client (self:client_id) = {

    # Subscribe to a particular key.
    export action subscribe_req(key:key_t)
    # In response, receive `content`, a log of all previous messages for this key.
    # Once this message is received,the client will also receive `publish_rsp`
    # when values are published to this key.
    import action subscribe_rsp(key:key_t,content:log_t)

    # Unsubscribe from a subscribed channel.
    export action unsubscribe_req(key:key_t)
    # In response, receive a notification that no more publishes will be sent.
    import action unsubscribe_rsp(key:key_t)

    # Publish a value to this key. Expected to only be called after a corresponding 
    # `subscribe_req` and `subscribe_rsp`.
    export action publish_req(key:key_t,val:val_t)
    # Callback to notify clients when a value has been published to a key they've 
    # subscribed to. This includes values published by themself.
    import action publish_rsp(key:key_t,val:val_t)

    specification {

        common {

            var pending_subscribe(X:client_id,K:key_t) : bool           # a subscribe is pending for this key
            var pending_unsubscribe(X:client_id,K:key_t) : bool         # an unsubscribe is pending for this key
            var subscribed(X:client_id,K:key_t) : bool                  # this client is subscribed to this key
            var n_commits(X:client_id,K:key_t) : commit_id              # the number of messages seen by this client
            var commit_order(X:client_id,K:key_t,C:commit_id) : val_t   # the commit order seen by each client to each key
            var n_pubs(X:client_id,K:key_t) : pub_id                    # the number of publications by this client
            var pending_pub(X:client_id,K:key_t,P:pub_id) : bool        # does a client have a pending publication for this?
            var pending_pub_val(X:client_id,K:key_t,P:pub_id) : val_t   # all the pending values published by each client to each key

            # Checks if a commit is consistent with our requirements.
            function valid_commit(K:key_t,C:commit_id,V:val_t) =
                                # The commit must agree with other clients' orders for this key.
                                (forall X:client_id. n_commits(X,K) > C -> commit_order(X,K,C) = V) &
                                # The commit must have come from somewhere.
                                (exists X:client_id,P:pub_id. (n_commits(X,K) > C & V.src = X) | 
                                                              (pending_pub(X,K,P) & pending_pub_val(X,K,P) = V))

            after init {
                pending_subscribe(X,K) := false;
                subscribed(X,K) := false;
                n_commits(X,K) := 0;
                n_pubs(X,K) := 0;
            }

            before subscribe_req(self:client_id,key:key_t) {
                # Client must only have one pending subscribe per key, 
                # and must not currently be subscribed to this key.
                require ~pending_subscribe(self,key) & ~subscribed(self,key);
                pending_subscribe(self,key) := true;
            }

            before subscribe_rsp(self:client_id,key:key_t,content:log_t) {
                # Client must have requested this subscription.
                require pending_subscribe(self,key);
                pending_subscribe(self,key) := false;
                subscribed(self,key) := true;
                # Add all entries to this client's observed commit order, 
                # and check that they match other clients'.
                var i : index; i := 0;
                var c : commit_id; c := 0;
                while i < content.end
                decreases content.end - i {
                    var val := content.get(i);
                    require valid_commit(key,c,val);
                    commit_order(self,key,c) := val;
                    i := i.next; c := c.next;
                }
                n_commits(self,key) := c;
            }

            before unsubscribe_req(self:client_id,key:key_t) {
                # Client must only have one pending unsubcribe per key, 
                # and must currently be subscribed to this key.
                require ~pending_unsubscribe(self,key) & subscribed(self,key);
                pending_unsubscribe(self,key) := true;
            }

            before unsubscribe_rsp(self:client_id,key:key_t) {
                # Client must have requested this unsubscription.
                require pending_unsubscribe(self,key);
                pending_unsubscribe(self,key) := false;
                subscribed(self,key) := false;
            }

            before publish_req(self:client_id,key:key_t,val:val_t) {
                # Client must be subscribed before it can publish.
                require subscribed(self,key);
                var p := n_pubs(self,key);
                pending_pub(self,key,p) := true;
                pending_pub_val(self,key,p) := val;
                n_pubs(self,key) := p.next;
            }

            before publish_rsp(self:client_id,key:key_t,val:val_t) {
                # Client must still be subscribed to this key.
                require subscribed(self,key);
                # Check that clients see sequential consistency for their own publications.
                if val.src = self {
                    var p := n_pubs(self,key);
                    # There must be a pending publish whose value must match this value, 
                    # which has no previous pending publishes, and all further pub_ids are pending.
                    require exists P:pub_id. pending_pub(self,key,P) & pending_pub_val(self,key,P) = val &
                                             forall Q. (Q < P -> ~pending_pub(self,key,Q)) & 
                                                       ((Q > P & Q < p) -> pending_pub(self,key,Q));
                    pending_pub(self,key,p) := false;
                    n_pubs(self,key) := p.next;
                }
                # Keep track of this publication, and make sure it matches other clients.
                var c := n_commits(self,key);
                require valid_commit(key,c,val);
                commit_order(self,key,c) := val;
                n_commits(self,key) := c.next;
            }
        }
    }

    implementation {

        common {

            # This type represents a range of keys and a list of key/value pairs in that range.
            global {
                instance shard_t : table_shard(key_t,channel_t)
            }

            # Describes a key's configuration.
            class config = {
                field primary : server_id   # the primary server for this key
                field secondary : server_id # the secondary server for this key
                field replicating : bool    # is this key being replicated? i.e. is it safe to serve requests?
            }

            # Clients and servers communicate using these message kinds:
            type msg_kind = {
                sub_req_kind,   # subscribe request
                sub_rsp_kind,   # subscribe response
                unsub_req_kind, # unsubscribe request
                unsub_rsp_kind, # unsubscribe response
                pub_req_kind,   # publish request
                pub_rsp_kind,   # publish response
                replicate_kind  # server-server log transfer
            }

            # Message class for client-server messages.
            class msg_t = {
                field kind : msg_kind           # kind of message
                field key : key_t               # key to operate on
                field val : val_t               # value, if any
                field log : log_t               # result of a subscribe request
                field src_client : client_id    # requesting client ID
                field primary : server_id       # who the primary is
                field shard : shard_t           # shard, only used for replicate messages
            }

            # Network for client-server messages.
            instance net : tcp.net(msg_t)

            # Management messages have the following structure:
            class man_msg_t = {
                field view : nat            # view number (indicates configuration version)
                field lo : key_t.iter.t     # lower bound of key range
                field hi : key_t.iter.t     # upper bound of key range
                field primary : server_id   # primary server id, if any
                field secondary : server_id # secondary server id, if any
                field src : server_id       # pinging server id, if any
            }

            # Network for management messages.
            instance man_net : tcp.net(man_msg_t)

            process manager(self:manager_id) = {

                # Manager state.
                var view : nat                  # the current view number
                var time : nat                  # the current time (in seconds)
                var heard(S:server_id) : nat    # last time we heard from server S
                var proposed : bool             # have we proposed a new view?
                instance confmap : delegation_map(key_t,config)    # stores configuration for each key

                common {
                    # Server failure timeout, can be overridden on command line.
                    parameter fail_time : nat = 2
                    # Paxos decides on this operation, but can optionally decide nothing.
                    instance op_t : option(man_msg_t)
                }

                # Manager's socket on management network.
                instance sock : man_net.socket

                # A timer that ticks once a second.
                instance timer : timeout_sec

                after init {
                    view := 0;
                    time := 0;
                    proposed := false;
                    var conf : config;
                    conf.primary := 0;
                    conf.secondary := 1;
                    conf.replicating := false;
                    confmap.set(key_t.iter.begin,key_t.iter.end,conf);
                }

                # A server is up if we've heard from it in the last `fail_time` seconds.
                function is_up(S:server_id) = time <= heard(S) + fail_time

                # Managers take commands to assign a range of keys `[lo,hi)` to a given primary and secondary.
                export action assign(lokey:key_t,hikey:key_t,primary:server_id,secondary:server_id) = {
                    if primary ~= secondary {
                        announce(view+1, key_t.iter.create(lokey), key_t.iter.create(hikey), primary, secondary);
                    }
                }

                # Propose a view change to Paxos. The view change will only be broadcast once decided by Paxos.
                action announce(view:nat,lo:key_t.iter.t,hi:key_t.iter.t,primary:server_id,secondary:server_id) = {
                    if ~proposed {
                        var msg : man_msg_t;
                        msg.view := view;
                        msg.lo := lo;
                        msg.hi := hi;
                        msg.primary := primary;
                        msg.secondary := secondary;
                        # Ask paxos to agree on our view change.
                        paxos.server.propose(client.manager.op_t.just(msg));
                        proposed := true;
                    }
                }

                # On a callback from Paxos, execute the view change.
                implement paxos.server.decide(inst:paxos.instance_t,op:op_t) {
                    proposed := false;
                    # Check that Paxos actually decided something.
                    if ~op.is_empty {
                        var msg := op.contents;
                        var conf : config;
                        conf.primary := msg.primary;
                        conf.secondary := msg.secondary;
                        var hi := msg.hi;
                        # Iterate over all shards in this range, and re-assign them.
                        while msg.lo < hi {
                            msg.hi := hi;
                            var lub := confmap.lub(msg.lo.next);
                            if lub < msg.hi {
                                msg.hi := lub;
                            }
                            assign_in_steps(msg);
                            confmap.set(msg.lo,msg.hi,conf);
                            msg.lo := msg.hi;
                        }
                    }
                }

                # Each view change is done in two steps to enable asynchrony:
                # 1. Make the new primary secondary.
                # 2. Make the secondary primary and replicate to new secondary.
                # The first step is only necessary if both the primary and secondary are changing.
                action assign_in_steps(msg:man_msg_t) = {
                    var old_config := confmap.get(msg.lo.val);
                    if old_config.primary ~= msg.primary & old_config.secondary ~= msg.primary {
                        var pmsg := msg;
                        pmsg.primary := old_config.primary;
                        pmsg.secondary := msg.primary;
                        broadcast(pmsg);
                    }
                    broadcast(msg);
                }

                # Broadcasts a new view message to all clients and servers.
                action broadcast(msg:man_msg_t) = {
                    view := view.next;
                    msg.view := view;
                    for it,cl in client_id.iter {
                        sock.send(client(cl).man_sock.id,msg);
                    }
                    for it,sv in server_id.iter {
                        sock.send(server(sv).man_sock.id,msg);
                    }
                }

                # Record time from server ping messages.
                implement sock.recv(src:tcp.endpoint,msg:man_msg_t) {
                    if msg.view = view {
                        heard(msg.src) := time;
                    }
                }

                # Paxos instantation.
                instance paxos : multi_paxos(manager_id,client.manager.op_t,client.manager.op_t.empty)

            } with client.manager.paxos.nset, manager.paxos.nset.api, manager.paxos.nset.majorities_intersect, client.manager.paxos.round_t, client.manager.paxos.instance_t, vector[client.manager.paxos.vote_t], client.manager.paxos, client.manager.paxos.server, nat

            process server(self:server_id) = {

                # Server network sockets.
                instance sock : net.socket
                instance man_sock : man_net.socket

                # Server state.
                instance table : hash_table(key_t,channel_t,shard_t) # K/V replica
                var view : nat                                          # view version
                instance confmap : delegation_map(key_t,config)         # configuration
                instance timer : timeout_sec                            # timer for heartbeat pings
                instance view_queue : unbounded_queue(man_msg_t)        # holds unprocessed view changes
                var replicating : bool                                  # is this server replicating new keys now?

                after init {
                    var conf : config;
                    conf.primary := 0;
                    conf.secondary := 1;
                    conf.replicating := false;
                    confmap.set(key_t.iter.begin,key_t.iter.end,conf);
                    view := 0;
                    replicating := false;
                }

                # Client-server network.
                implement sock.recv(src:tcp.endpoint,msg:msg_t) {
                    if msg.kind = sub_req_kind | msg.kind = unsub_req_kind {
                        var conf := confmap.get(msg.key);
                        var sub := msg.kind = sub_req_kind;
                        if conf.primary = self {
                            # Record the subscribe/unsubscribe, then forward to secondary, who will commit and get back to us.
                            var chan := table.get(msg.key);
                            #chan.client_subscribed(msg.src_client) := sub;
                            # Look for this client in subscribed clients.
                            var clients := chan.clients_subscribed;
                            var i : index; i := 0;
                            var found := false;
                            var found_idx : index;
                            while i < clients.end {
                                var c := clients.get(i);
                                if c = msg.src_client {
                                    found := true;
                                    found_idx := i;
                                }
                                i := i.next;
                            }
                            # Complete the operation, if necessary.
                            var complete_op := false;
                            if sub & ~found {
                                # A new client is subscribing.
                                chan.clients_subscribed := clients.append(msg.src_client);
                                complete_op := true;
                            } else if ~sub & found {
                                # An existing client is unsubscribing.
                                var new_clients : client_list;
                                if found_idx ~= 0 {
                                    new_clients := new_clients.extend(clients.segment(0,found_idx));
                                }
                                if found_idx ~= clients.size() {
                                    new_clients := new_clients.extend(clients.segment(found_idx+1,clients.size()));
                                }
                                chan.clients_subscribed := new_clients;
                                complete_op := true;
                            }
                            if complete_op {
                                table.set(msg.key,chan);
                                sock.send(server(conf.secondary).sock.id,msg);
                            }
                        } else if conf.secondary = self & ~conf.replicating {
                            # If we are the secondary and are not replicating, record the subscribe, and respond to the primary.
                            if src = server(conf.primary).sock.id {
                                var chan := table.get(msg.key);
                                #chan.client_subscribed(msg.src_client) := sub;
                                table.set(msg.key,chan);
                                msg.kind := sub_rsp_kind;
                                msg.log := chan.log;
                                sock.send(server(conf.primary).sock.id,msg);
                            }
                        }
                    } else if msg.kind = sub_rsp_kind | msg.kind = unsub_rsp_kind {
                        # The secondary has completed the subscribe/unsubscribe, so now it's safe to respond to the client.
                        sock.send(client(msg.src_client).sock.id,msg);
                    } else if msg.kind = pub_req_kind {
                        # TODO
                    } else if msg.kind = replicate_kind {
                        # A new primary has sent us their data to replicate it.
                        var conf := confmap.get(msg.key);
                        table.incorporate(msg.shard);
                        conf.secondary := self;
                        conf.primary := msg.primary;
                        conf.replicating := false;
                        confmap.set(msg.shard.lo,msg.shard.hi,conf);
                        replicating := false;
                        # Process any view changes we ignored while waiting to replicate.
                        while ~view_queue.empty & ~replicating {
                            process_view(view_queue.dequeue);
                        }
                    }
                }

                action process_view(msg:man_msg_t) = {
                    # Ignore duplicates resulting from multi-Paxos.
                    if msg.view = view+1 {
                        var conf := confmap.get(msg.lo.val);
                        if msg.primary = self & conf.secondary ~= msg.secondary {
                            # We are the new primary, so the send the new secondary our data.
                            var rmsg : msg_t;
                            rmsg.kind := replicate_kind;
                            rmsg.shard := table.extract_(msg.lo,msg.hi);
                            rmsg.primary := self;
                            sock.send(server(msg.secondary).sock.id,rmsg);
                        }
                        if self = msg.secondary & self ~= conf.secondary {
                            # If we are the new secondary, wait for a replicate message.
                            conf.replicating := true;
                            replicating := true;
                        }
                        # Update our state.
                        view := msg.view;
                        conf.primary := msg.primary;
                        conf.secondary := msg.secondary;
                        confmap.set(msg.lo,msg.hi,conf);
                    }
                }

                implement man_sock.recv(src:tcp.endpoint,msg:man_msg_t) {
                    # Management message informing us of view change.
                    if replicating {
                        # We're in the middle of another replicate operation, so hold off on this one.
                        view_queue.enqueue(msg);
                    } else {
                        # Otherwise, we can handle the new view.
                        process_view(msg);
                    }
                }
            }
        }

        #
        # Client implementation.
        #

        # Client state.
        instance confmap : delegation_map(key_t,config)

        # Cliet network sockets.
        instance sock : net.socket
        instance man_sock : man_net.socket

        after init {
            var conf : config;
            conf.primary := 0;
            conf.secondary := 1;
            conf.replicating := false;
            confmap.set(key_t.iter.begin,key_t.iter.end,conf);
        }

        implement subscribe_req {
            var msg : msg_t;
            msg.kind := sub_req_kind;
            msg.src_client := self;
            msg.key := key;
            var conf := confmap.get(key);
            var primary := conf.primary;
            sock.send(server(primary).sock.id,msg);
        }

        implement unsubscribe_req {
            var msg : msg_t;
            msg.kind := unsub_req_kind;
            msg.src_client := self;
            msg.key := key;
            var conf := confmap.get(key);
            var primary := conf.primary;
            sock.send(server(primary).sock.id,msg);
        }

        implement publish_req {
            var msg : msg_t;
            msg.kind := pub_req_kind;
            msg.src_client := self;
            msg.key := key;
            msg.val := val;
            var conf := confmap.get(key);
            var primary := conf.primary;
            sock.send(server(primary).sock.id,msg);
        }

        implement sock.recv(src:tcp.endpoint,msg:msg_t) {
            if msg.kind = sub_rsp_kind {
                subscribe_rsp(msg.key,msg.log);
            } else if msg.kind = unsub_rsp_kind {
                unsubscribe_rsp(msg.key);
            } else if msg.kind = pub_rsp_kind {
                publish_rsp(msg.key,msg.val);
            }
        }

        implement man_sock.recv(src:tcp.endpoint,msg:man_msg_t) {
            var conf : config;
            conf.primary := msg.primary;
            conf.secondary := msg.secondary;
            confmap.set(msg.lo,msg.hi,conf);
        }
    }
}
