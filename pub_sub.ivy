#lang ivy1.8

# STL
include numbers
include network
include order
include collections
include timeout

# Project-specific
include arrayset
include shard
include table
include delmap
include multi_paxos

# TODO: timeout mechanism, publishes
include timeout

# Types.
global {
    # IDs for different processes.
    instance client_id : iterable
    instance server_id : iterable
    instance manager_id : iterable

    # Key type used to identify pub/sub channels.
    instance key_t : iterable

    # Value types used in channels.
    alias byte = uint[8]
    alias data_t = byte
    class val_t = {
        field data : data_t
        field src : client_id
    }
    instance log_t : vector(val_t)  # a history of channel messages
    instance client_set : arrayset(client_id)

    # Channel state.
    class channel_t = {
        field log : log_t # channel message history
        #field client_subscribed(C:client_id) : bool # set of clients subscribed to this channel
        field clients_subscribed : client_set # set of clients subscribed to this channel
    }

    # Specification types.
    instance pub_id : iterable
}

process client (self:client_id) = {

    # Subscribe to a particular key.
    export action subscribe_req(key:key_t)
    # In response, receive `content`, a log of all previous messages for this key.
    # Once this message is received,the client will also receive `publish_rsp`
    # when values are published to this key.
    import action subscribe_rsp(key:key_t,content:log_t)

    # Unsubscribe from a subscribed channel.
    export action unsubscribe_req(key:key_t)
    # In response, receive a notification that no more publishes will be sent.
    import action unsubscribe_rsp(key:key_t)

    # Publish a value to this key. Expected to only be called after a corresponding 
    # `subscribe_req` and `subscribe_rsp`.
    export action publish_req(key:key_t,data:data_t)
    # Callback to notify clients when a value has been published to a key they've 
    # subscribed to. This includes values published by themself.
    import action publish_rsp(key:key_t,val:val_t)

    specification {

        common {

            var pending_subscribe(X:client_id,K:key_t) : bool           # a subscribe is pending for this key
            var pending_unsubscribe(X:client_id,K:key_t) : bool         # an unsubscribe is pending for this key
            var subscribed(X:client_id,K:key_t) : bool                  # this client is subscribed to this key
            var n_commits(X:client_id,K:key_t) : pub_id                 # the total number of publications seen by this client
            var commit_order(X:client_id,K:key_t,C:pub_id) : val_t      # the total publication order seen by each client to each key
            var n_pubs(X:client_id,K:key_t) : pub_id                    # the number of publications requested by this client
            var n_pubs_committed(X:client_id,K:key_t) : pub_id          # the number of publications "committed" that were requested by this client
            var pending_pub(X:client_id,K:key_t,P:pub_id) : bool        # does a client have a pending publication for this?
            var pub_committed(X:client_id,K:key_t,P:pub_id) : bool      # has this client's publication been committed yet?
            var pending_pub_data(X:client_id,K:key_t,P:pub_id) : data_t # all the pending values published by each client to each key

            # Checks if a commit is consistent with our requirements.
            function valid_commit(K:key_t,P:pub_id,V:val_t) =
                                # The commit must agree with other clients' orders for this key.
                                # Basically, everyone agrees on *some* order for the channel.
                                (forall X:client_id. n_commits(X,K) > P -> commit_order(X,K,P) = V) &
                                # The commit must have come from somewhere.
                                (exists X:client_id,P:pub_id. (n_commits(X,K) > P & V.src = X) | 
                                                              ((pending_pub(X,K,P) | pub_committed(X,K,P)) & pending_pub_data(X,K,P) = V.data))

            after init {
                pending_subscribe(X,K) := false;
                pending_unsubscribe(X,K) := false;
                subscribed(X,K) := false;
                n_commits(X,K) := 0;
                n_pubs(X,K) := 0;
                n_pubs_committed(X,K) := 0;
            }

            before subscribe_req(self:client_id,key:key_t) {
                # Client must only have one pending subscribe per key, 
                # and must not currently be subscribed to this key.
                require ~pending_subscribe(self,key) & ~subscribed(self,key);
                pending_subscribe(self,key) := true;
            }

            before subscribe_rsp(self:client_id,key:key_t,content:log_t) {
                # Client must have requested this subscription.
                require pending_subscribe(self,key);
                pending_subscribe(self,key) := false;
                subscribed(self,key) := true;
                # Add all entries to this client's observed commit order, 
                # and check that they match other clients'.
                var i : index; i := 0;
                var c : pub_id; c := 0;
                while i < content.end
                decreases content.end - i {
                    var val := content.get(i);
                    require valid_commit(key,c,val);
                    commit_order(self,key,c) := val;
                    i := i.next; c := c.next;
                }
                n_commits(self,key) := c;
            }

            before unsubscribe_req(self:client_id,key:key_t) {
                # Client must only have one pending unsubcribe per key, 
                # and must currently be subscribed to this key.
                require ~pending_unsubscribe(self,key) & subscribed(self,key);
                pending_unsubscribe(self,key) := true;
            }

            before unsubscribe_rsp(self:client_id,key:key_t) {
                # Client must have requested this unsubscription.
                require pending_unsubscribe(self,key);
                pending_unsubscribe(self,key) := false;
                subscribed(self,key) := false;
            }

            before publish_req(self:client_id,key:key_t,data:data_t) {
                # Client must be subscribed and not be unsubscribing to publish.
                require subscribed(self,key) & ~pending_unsubscribe(self,key);
                var p := n_pubs(self,key);
                pending_pub(self,key,p) := true;
                var val : val_t;
                val.data := data;
                val.src := self;
                pending_pub_data(self,key,p) := data;
                n_pubs(self,key) := p.next;
            }

            before publish_rsp(self:client_id,key:key_t,val:val_t) {
                # Client must still be subscribed to this key.
                require subscribed(self,key);
                # Keep track of this publication, and make sure it matches other clients.
                var c := n_commits(self,key);
                # Consistency check for self-publications done in `commit`, 
                # so this check is only for other clients' commits.
                if val.src ~= self {
                    require valid_commit(key,c,val);
                }
                commit_order(self,key,c) := val;
                n_commits(self,key) := c.next;
            }

            # Called when a client's publish has been "committed" server-side.
            # Here, I check that the publication is sequentially consistent.
            action commit(key:key_t,self:client_id,data:data_t) = {
                var p_lo := n_pubs_committed(self,key);
                var p_hi:= n_pubs(self,key);
                debug "commit" with key=key, client=self, data=data, p_lo=p_lo, p_hi=p_hi;
                # There must be a pending publish whose value must match this value, 
                # which has no previous pending publishes, and all further pub_ids are pending.
                require exists P:pub_id. pending_pub(self,key,P) & pending_pub_data(self,key,P) = data &
                                         forall Q:pub_id. (Q < P -> ~pending_pub(self,key,Q)) & 
                                                          ((Q > P & Q < p_hi) -> pending_pub(self,key,Q));

                pending_pub(self,key,p_lo) := false;
                pub_committed(self,key,p_lo) := true;
                n_pubs_committed(self,key) := p_lo.next;
            }

        }
    }

    implementation {

        common {

            # This type represents a range of keys and a list of key/value pairs in that range.
            global {
                instance shard_t : table_shard(key_t,channel_t)
            }

            # Describes a key's configuration.
            class config = {
                field primary : server_id   # the primary server for this key
                field secondary : server_id # the secondary server for this key
                field replicating : bool    # is this key being replicated? i.e. is it safe to serve requests?
            }

            # Clients and servers communicate using these message kinds:
            type msg_kind = {
                sub_req_kind,   # subscribe request
                sub_rsp_kind,   # subscribe response
                unsub_req_kind, # unsubscribe request
                unsub_rsp_kind, # unsubscribe response
                pub_req_kind,   # publish request
                pub_rsp_kind,   # publish response
                replicate_kind  # server-server log transfer
            }

            # Message class for client-server messages.
            class msg_t = {
                field kind : msg_kind           # kind of message
                field key : key_t               # key to operate on
                field val : val_t               # value, if any
                field log : log_t               # result of a subscribe request
                field src_client : client_id    # requesting client ID
                field primary : server_id       # who the primary is
                field shard : shard_t           # shard, only used for replicate messages
                field clients : client_set      # set of subscribed clients to send a message to, 
                                                # only used to maintain consistency when handling 
                                                # a publish request
            }

            # Network for client-server messages.
            instance net : tcp.net(msg_t)

            # Management messages have the following structure:
            class man_msg_t = {
                field view : nat            # view number (indicates configuration version)
                field lo : key_t.iter.t     # lower bound of key range
                field hi : key_t.iter.t     # upper bound of key range
                field primary : server_id   # primary server id, if any
                field secondary : server_id # secondary server id, if any
                field src : server_id       # pinging server id, if any
            }

            # Network for management messages.
            instance man_net : tcp.net(man_msg_t)

            process manager(self:manager_id) = {

                # Manager state.
                var view : nat                  # the current view number
                var time : nat                  # the current time (in seconds)
                var heard(S:server_id) : nat    # last time we heard from server S
                var proposed : bool             # have we proposed a new view?
                instance confmap : delegation_map(key_t,config)    # stores configuration for each key

                common {
                    # Server failure timeout, can be overridden on command line.
                    parameter fail_time : nat = 2
                    # Paxos decides on this operation, but can optionally decide nothing.
                    instance op_t : option(man_msg_t)
                }

                # Manager's socket on management network.
                instance sock : man_net.socket

                # A timer that ticks once a second.
                instance timer : timeout_sec

                after init {
                    view := 0;
                    time := 0;
                    proposed := false;
                    var conf : config;
                    conf.primary := 0;
                    conf.secondary := 1;
                    conf.replicating := false;
                    confmap.set(key_t.iter.begin,key_t.iter.end,conf);
                }

                # A server is up if we've heard from it in the last `fail_time` seconds.
                function is_up(S:server_id) = time <= heard(S) + fail_time

                # Managers take commands to assign a range of keys `[lo,hi)` to a given primary and secondary.
                export action assign(lokey:key_t,hikey:key_t,primary:server_id,secondary:server_id) = {
                    if primary ~= secondary {
                        announce(view+1, key_t.iter.create(lokey), key_t.iter.create(hikey), primary, secondary);
                    }
                }

                # Propose a view change to Paxos. The view change will only be broadcast once decided by Paxos.
                action announce(view:nat,lo:key_t.iter.t,hi:key_t.iter.t,primary:server_id,secondary:server_id) = {
                    if ~proposed {
                        var msg : man_msg_t;
                        msg.view := view;
                        msg.lo := lo;
                        msg.hi := hi;
                        msg.primary := primary;
                        msg.secondary := secondary;
                        # Ask paxos to agree on our view change.
                        paxos.server.propose(client.manager.op_t.just(msg));
                        proposed := true;
                    }
                }

                # On a callback from Paxos, execute the view change.
                implement paxos.server.decide(inst:paxos.instance_t,op:op_t) {
                    proposed := false;
                    # Check that Paxos actually decided something.
                    if ~op.is_empty {
                        var msg := op.contents;
                        var conf : config;
                        conf.primary := msg.primary;
                        conf.secondary := msg.secondary;
                        var hi := msg.hi;
                        # Iterate over all shards in this range, and re-assign them.
                        while msg.lo < hi {
                            msg.hi := hi;
                            var lub := confmap.lub(msg.lo.next);
                            if lub < msg.hi {
                                msg.hi := lub;
                            }
                            assign_in_steps(msg);
                            confmap.set(msg.lo,msg.hi,conf);
                            msg.lo := msg.hi;
                        }
                    }
                }

                # Each view change is done in two steps to enable asynchrony:
                # 1. Make the new primary secondary.
                # 2. Make the secondary primary and replicate to new secondary.
                # The first step is only necessary if both the primary and secondary are changing.
                action assign_in_steps(msg:man_msg_t) = {
                    var old_config := confmap.get(msg.lo.val);
                    if old_config.primary ~= msg.primary & old_config.secondary ~= msg.primary {
                        var pmsg := msg;
                        pmsg.primary := old_config.primary;
                        pmsg.secondary := msg.primary;
                        broadcast(pmsg);
                    }
                    broadcast(msg);
                }

                # Broadcasts a new view message to all clients and servers.
                action broadcast(msg:man_msg_t) = {
                    view := view.next;
                    msg.view := view;
                    for it,cl in client_id.iter {
                        sock.send(client(cl).man_sock.id,msg);
                    }
                    for it,sv in server_id.iter {
                        sock.send(server(sv).man_sock.id,msg);
                    }
                }

                # Record time from server ping messages.
                implement sock.recv(src:tcp.endpoint,msg:man_msg_t) {
                    if msg.view = view {
                        heard(msg.src) := time;
                    }
                }

                # Paxos instantation.
                instance paxos : multi_paxos(manager_id,client.manager.op_t,client.manager.op_t.empty)

            } with client.manager.paxos.nset, manager.paxos.nset.api, manager.paxos.nset.majorities_intersect, client.manager.paxos.round_t, client.manager.paxos.instance_t, vector[client.manager.paxos.vote_t], client.manager.paxos, client.manager.paxos.server, nat

            process server(self:server_id) = {

                # Server network sockets.
                instance sock : net.socket
                instance man_sock : man_net.socket

                # Server state.
                instance table : hash_table(key_t,channel_t,shard_t) # K/V replica
                var view : nat                                          # view version
                instance confmap : delegation_map(key_t,config)         # configuration
                instance timer : timeout_sec                            # timer for heartbeat pings
                instance view_queue : unbounded_queue(man_msg_t)        # holds unprocessed view changes
                var replicating : bool                                  # is this server replicating new keys now?

                after init {
                    var conf : config;
                    conf.primary := 0;
                    conf.secondary := 1;
                    conf.replicating := false;
                    confmap.set(key_t.iter.begin,key_t.iter.end,conf);
                    view := 0;
                    replicating := false;
                }

                # Client-server network.
                implement sock.recv(src:tcp.endpoint,msg:msg_t) {
                    if msg.kind = sub_req_kind | msg.kind = unsub_req_kind {
                        var conf := confmap.get(msg.key);
                        var sub := msg.kind = sub_req_kind;
                        if conf.primary = self {
                            # Record the subscribe/unsubscribe, then forward to secondary, who will commit and get back to us.
                            var chan := table.get(msg.key);
                            if sub {
                                # A new client is subscribing.
                                chan.clients_subscribed := chan.clients_subscribed.add(msg.src_client);
                            } else {
                                # An existing client is unsubscribing.
                                chan.clients_subscribed := chan.clients_subscribed.remove(msg.src_client);
                            }
                            table.set(msg.key,chan);
                            sock.send(server(conf.secondary).sock.id,msg);
                        } else if conf.secondary = self & ~conf.replicating {
                            # If we are the secondary and are not replicating, record the subscribe/unsubscribe, and respond to the primary.
                            if src = server(conf.primary).sock.id {
                                var chan := table.get(msg.key);
                                if sub {
                                    # A new client is subscribing.
                                    chan.clients_subscribed := chan.clients_subscribed.add(msg.src_client);
                                    msg.kind := sub_rsp_kind;
                                } else {
                                    # An existing client is unsubscribing.
                                    chan.clients_subscribed := chan.clients_subscribed.remove(msg.src_client);
                                    msg.kind := unsub_rsp_kind;
                                }
                                table.set(msg.key,chan);
                                msg.log := chan.log;
                                sock.send(server(conf.primary).sock.id,msg);
                            }
                        }
                    } else if msg.kind = sub_rsp_kind | msg.kind = unsub_rsp_kind {
                        # The secondary has completed the subscribe/unsubscribe, so now it's safe to respond to the client.
                        sock.send(client(msg.src_client).sock.id,msg);
                    } else if msg.kind = pub_req_kind {
                        var conf := confmap.get(msg.key);
                        if conf.primary = self {
                            var chan := table.get(msg.key);
                            chan.log := chan.log.append(msg.val);
                            table.set(msg.key,chan);
                            sock.send(server(conf.secondary).sock.id,msg);
                            # Receiving this request has serialized its order.
                            serialize(msg.key,self,conf.secondary,msg.src_client,msg.val.data);
                        } else if conf.secondary = self & ~conf.replicating {
                            if src = server(conf.primary).sock.id {
                                var chan := table.get(msg.key);
                                chan.log := chan.log.append(msg.val);
                                table.set(msg.key,chan);
                                msg.kind := pub_rsp_kind;
                                msg.clients := chan.clients_subscribed;
                                sock.send(server(conf.primary).sock.id,msg);
                                # This publish is now effectively "committed".
                                commit_one(msg.key,msg.src_client);
                            }
                        }
                    } else if msg.kind = pub_rsp_kind {
                        # Broadcast the publish to all subscribed clients, 
                        # as determined by the secondary.
                        for it,cl in client_id.iter {
                            if msg.clients.contains(cl) {
                                sock.send(client(cl).sock.id,msg);
                            }
                        }
                    } else if msg.kind = replicate_kind {
                        # A new primary has sent us their data to replicate it.
                        var conf := confmap.get(msg.key);
                        table.incorporate(msg.shard);
                        conf.secondary := self;
                        conf.primary := msg.primary;
                        conf.replicating := false;
                        confmap.set(msg.shard.lo,msg.shard.hi,conf);
                        replicating := false;
                        # Process any view changes we ignored while waiting to replicate.
                        while ~view_queue.empty & ~replicating {
                            process_view(view_queue.dequeue);
                        }
                    }
                }

                action process_view(msg:man_msg_t) = {
                    # Ignore duplicates resulting from multi-Paxos.
                    if msg.view = view+1 {
                        var conf := confmap.get(msg.lo.val);
                        if msg.primary = self & conf.secondary ~= msg.secondary {
                            # We are the new primary, so the send the new secondary our data.
                            debug "becoming primary" with server=self;
                            var rmsg : msg_t;
                            rmsg.kind := replicate_kind;
                            rmsg.shard := table.extract_(msg.lo,msg.hi);
                            rmsg.primary := self;
                            sock.send(server(msg.secondary).sock.id,rmsg);
                            # Call the ghost action to commit any outstanding pubs for this key.
                            var it := msg.lo;
                            while it < msg.hi {
                                commit_all_serialized(it.val,self,conf.secondary);
                                it := it.next;
                            }
                        }
                        if self = msg.secondary & self ~= conf.secondary {
                            # If we are the new secondary, wait for a replicate message.
                            debug "becoming secondary" with server=self;
                            conf.replicating := true;
                            replicating := true;
                        }
                        # Update our state.
                        view := msg.view;
                        conf.primary := msg.primary;
                        conf.secondary := msg.secondary;
                        confmap.set(msg.lo,msg.hi,conf);
                    }
                }

                # Management message informing us of view change.
                implement man_sock.recv(src:tcp.endpoint,msg:man_msg_t) {
                    if replicating {
                        # We're in the middle of another replicate operation, so hold off on this one.
                        view_queue.enqueue(msg);
                    } else {
                        # Otherwise, we can handle the new view.
                        process_view(msg);
                    }
                }

                # Ghost code to keep track of publish orders (per key and client)
                # to check sequential consistency.
                specification {
                    # Keeps track of publish requests serialized at the primary, 
                    # but not committed by the secondary.
                    instance serialized(K:key_t,C:client_id) : unbounded_queue(data_t)

                    # Called by the secondary to commit a publish.
                    action commit_one(k:key_t,src_client:client_id) = {
                        if ~serialized(k,src_client).empty {
                            commit(k,src_client,serialized(k,src_client).dequeue);
                        }
                    }

                    common {
                        var true_primary(K:key_t) : server_id

                        # Initially, server 0 is the primary for all keys.
                        after init {
                            true_primary(K) := 0;
                        }

                        # This request has arrived at the primary, meaning its relative order is fixed/serialized.
                        action serialize(k:key_t,primary:server_id,secondary:server_id,src_client:client_id,data:data_t) = {
                            if primary = true_primary(k) {
                                serialized(secondary,k,src_client).enqueue(data);
                            }
                        }

                        # After replicating, this ghost action is called to commit any outstanding publishes.
                        action commit_all_serialized(k:key_t,self:server_id,secondary:server_id) = {
                            for it,src_client in client_id.iter {
                                while ~serialized(secondary,k,src_client).empty 
                                decreases serialized(secondary,k,src_client).tail - serialized(secondary,k,src_client).head {
                                    var data := serialized(secondary,k,src_client).dequeue;
                                    if self = true_primary(k) {
                                        commit(k,src_client,data);
                                    }
                                }
                                true_primary(k) := self;
                            }
                        }
                    }
                }
            }
        }

        #
        # Client implementation.
        #

        # Client state.
        instance confmap : delegation_map(key_t,config)

        # Cliet network sockets.
        instance sock : net.socket
        instance man_sock : man_net.socket

        after init {
            var conf : config;
            conf.primary := 0;
            conf.secondary := 1;
            conf.replicating := false;
            confmap.set(key_t.iter.begin,key_t.iter.end,conf);
        }

        implement subscribe_req {
            var msg : msg_t;
            msg.kind := sub_req_kind;
            msg.src_client := self;
            msg.key := key;
            var conf := confmap.get(key);
            var primary := conf.primary;
            sock.send(server(primary).sock.id,msg);
        }

        implement unsubscribe_req {
            var msg : msg_t;
            msg.kind := unsub_req_kind;
            msg.src_client := self;
            msg.key := key;
            var conf := confmap.get(key);
            var primary := conf.primary;
            sock.send(server(primary).sock.id,msg);
        }

        implement publish_req {
            var val : val_t;
            val.data := data;
            val.src := self;
            var msg : msg_t;
            msg.kind := pub_req_kind;
            msg.src_client := self;
            msg.key := key;
            msg.val := val;
            var conf := confmap.get(key);
            var primary := conf.primary;
            sock.send(server(primary).sock.id,msg);
        }

        implement sock.recv(src:tcp.endpoint,msg:msg_t) {
            if msg.kind = sub_rsp_kind {
                subscribe_rsp(msg.key,msg.log);
            } else if msg.kind = unsub_rsp_kind {
                unsubscribe_rsp(msg.key);
            } else if msg.kind = pub_rsp_kind {
                publish_rsp(msg.key,msg.val);
            }
        }

        implement man_sock.recv(src:tcp.endpoint,msg:man_msg_t) {
            var conf : config;
            conf.primary := msg.primary;
            conf.secondary := msg.secondary;
            confmap.set(msg.lo,msg.hi,conf);
        }
    }
}
