#lang ivy1.8

# STL
include numbers
include network
include order
include collections
include timeout

# Project-specific
include shard
include table
include delmap
include multi_paxos

# Types.
global {
    # IDs for different processes.
    instance client_id : iterable
    instance server_id : iterable
    instance manager_id : iterable

    # Key type used to identify pub/sub channels.
    instance key_t : iterable

    # Value types used in channels.
    alias byte = uint[8]
    alias data_t = byte
    class val_t = {
        field data : data_t
        field src : client_id
    }
    instance log_t : vector(val_t)

    # Specification types.
    instance pub_id : iterable
    instance commit_id : iterable
}

process client (self:client_id) = {

    # Subscribe to a particular key.
    export action subscribe_req(key:key_t)
    # In response, receive `content`, a log of all previous messages for this key.
    # Once this message is received,the client will also receive `publish_rsp`
    # when values are published to this key.
    import action subscribe_rsp(key:key_t,content:log_t)

    # Unsubscribe from a subscribed channel.
    export action unsubscribe_req(key:key_t)
    # In response, receive a notification that no more publishes will be sent.
    import action unsubscribe_rsp(key:key_t)

    # Publish a value to this key. Expected to only be called after a corresponding 
    # `subscribe_req` and `subscribe_rsp`.
    export action publish_req(key:key_t,val:val_t)
    # Callback to notify clients when a value has been published to a key they've 
    # subscribed to. This includes values published by themself.
    import action publish_rsp(key:key_t,val:val_t)

    specification {

        common {

            var pending_subscribe(X:client_id,K:key_t) : bool           # a subscribe is pending for this key
            var pending_unsubscribe(X:client_id,K:key_t) : bool         # an unsubscribe is pending for this key
            var subscribed(X:client_id,K:key_t) : bool                  # this client is subscribed to this key
            var n_commits(X:client_id,K:key_t) : commit_id              # the number of messages seen by this client
            var commit_order(X:client_id,K:key_t,C:commit_id) : val_t   # the commit order seen by each client to each key
            var n_pubs(X:client_id,K:key_t) : pub_id                    # the number of publications by this client
            var pending_pub(X:client_id,K:key_t,P:pub_id) : bool        # does a client have a pending publication for this?
            var pending_pub_val(X:client_id,K:key_t,P:pub_id) : val_t   # all the pending values published by each client to each key

            # Checks if a commit is consistent with our requirements.
            function valid_commit(K:key_t,C:commit_id,V:val_t) =
                                # The commit must agree with other clients' orders for this key.
                                (forall X:client_id. n_commits(X,K) > C -> commit_order(X,K,C) = V) &
                                # The commit must have come from somewhere.
                                (exists X:client_id,P:pub_id. (n_commits(X,K) > C & V.src = X) | 
                                                              (pending_pub(X,K,P) & pending_pub_val(X,K,P) = V))

            after init {
                pending_subscribe(X,K) := false;
                subscribed(X,K) := false;
                n_commits(X,K) := 0;
                n_pubs(X,K) := 0;
            }

            before subscribe_req(self:client_id,key:key_t) {
                # Client must only have one pending subscribe per key, 
                # and must not currently be subscribed to this key.
                require ~pending_subscribe(self,key) & ~subscribed(self,key);
                pending_subscribe(self,key) := true;
            }

            before subscribe_rsp(self:client_id,key:key_t,content:log_t) {
                # Client must have requested this subscription.
                require pending_subscribe(self,key);
                pending_subscribe(self,key) := false;
                subscribed(self,key) := true;
                # Add all entries to this client's observed commit order, 
                # and check that they match other clients'.
                var i : index; i := 0;
                var c : commit_id; c := 0;
                while i < content.end
                decreases content.end - i {
                    var val := content.get(i);
                    require valid_commit(key,c,val);
                    commit_order(self,key,c) := val;
                    i := i.next; c := c.next;
                }
                n_commits(self,key) := c;
            }

            before unsubscribe_req(self:client_id,key:key_t) {
                # Client must only have one pending unsubcribe per key, 
                # and must currently be subscribed to this key.
                require ~pending_unsubscribe(self,key) & subscribed(self,key);
                pending_unsubscribe(self,key) := true;
            }

            before unsubscribe_rsp(self:client_id,key:key_t) {
                # Client must have requested this unsubscription.
                require pending_unsubscribe(self,key);
                pending_unsubscribe(self,key) := false;
                subscribed(self,key) := false;
            }

            before publish_req(self:client_id,key:key_t,val:val_t) {
                # Client must be subscribed before it can publish.
                require subscribed(self,key);
                var p := n_pubs(self,key);
                pending_pub(self,key,p) := true;
                pending_pub_val(self,key,p) := val;
                n_pubs(self,key) := p.next;
            }

            before publish_rsp(self:client_id,key:key_t,val:val_t) {
                # Client must still be subscribed to this key.
                require subscribed(self,key);
                # Check that clients see sequential consistency for their own publications.
                if val.src = self {
                    var p := n_pubs(self,key);
                    # There must be a pending publish whose value must match this value, 
                    # which has no previous pending publishes, and all further pub_ids are pending.
                    require exists P:pub_id. pending_pub(self,key,P) & pending_pub_val(self,key,P) = val &
                                             forall Q. (Q < P -> ~pending_pub(self,key,Q)) & 
                                                       ((Q > P & Q < p) -> pending_pub(self,key,Q));
                    pending_pub(self,key,p) := false;
                    n_pubs(self,key) := p.next;
                }
                # Keep track of this publication, and make sure it matches other clients.
                var c := n_commits(self,key);
                require valid_commit(key,c,val);
                commit_order(self,key,c) := val;
                n_commits(self,key) := c.next;
            }
        }
    }

    implementation {

        common {

            # This type represents a range of keys and a list of key/value pairs in that range.
            global {
                instance shard_t : table_shard(key_t,log_t)
            }

            # Describes a key's configuration.
            class config = {
                field primary : server_id   # the primary server for this key
                field secondary : server_id # the secondary server for this key
                field replicating : bool    # is this key being replicated? i.e. is it safe to serve requests?
            }

            # Clients and servers communicate using these message kinds:
            type msg_kind = {
                sub_req_kind,   # subscribe request
                sub_rsp_kind,   # subscribe response
                unsub_req_kind, # unsubscribe request
                unsub_rsp_kind, # unsubscribe response
                pub_req_kind,   # publish request
                pub_rsp_kind    # publish response
            }

            # Message class for client-server messages.
            class msg_t = {
                field kind : msg_kind   # kind of message
                field key : key_t       # key to operate on
                field val : val_t       # value, if any
                field log : log_t       # result of a subscribe request
            }

            # Network for client-server messages.
            instance net : tcp.net(msg_t)

            # Management messages have the following structure:
            class man_msg_t = {
                field view : nat            # view number (indicates configuration version)
                field lo : key_t.iter.t     # lower bound of key range
                field hi : key_t.iter.t     # upper bound of key range
                field primary : server_id   # primary server id, if any
                field secondary : server_id # secondary server id, if any
                field src : server_id       # pinging server id, if any
            }

            # Network for management messages.
            instance man_net : tcp.net(man_msg_t)

            process manager(self:manager_id) = {

                # Manager state.
                var view : nat                  # the current view number
                var time : nat                  # the current time (in seconds)
                var heard(S:server_id) : nat    # last time we heard from server S
                var proposed : bool             # have we proposed a new view?
                instance confmap : delegation_map(key_t,config)    # stores configuration for each key

                common {
                    # Server failure timeout, can be overridden on command line.
                    parameter fail_time : nat = 2
                    # Paxos decides on this operation, but can optionally decide nothing.
                    instance op_t : option(man_msg_t)
                }

                # Manager's socket on management network.
                instance sock : man_net.socket

                # A timer that ticks once a second.
                instance timer : timeout_sec

                after init {
                    view := 0;
                    time := 0;
                    proposed := false;
                    var conf : config;
                    conf.primary := 0;
                    conf.secondary := 1;
                    conf.replicating := false;
                    confmap.set(key_t.iter.begin,key_t.iter.end,conf);
                }

                # A server is up if we've heard from it in the last `fail_time` seconds.
                function is_up(S:server_id) = time <= heard(S) + fail_time

                # Managers take commands to assign a range of keys `[lo,hi)` to a given primary and secondary.
                export action assign(lokey:key_t,hikey:key_t,primary:server_id,secondary:server_id) = {
                    if primary ~= secondary {
                        announce(view+1, key_t.iter.create(lokey), key_t.iter.create(hikey), primary, secondary);
                    }
                }

                # Propose a view change to Paxos. The view change will only be broadcast once decided by Paxos.
                action announce(view:nat,lo:key_t.iter.t,hi:key_t.iter.t,primary:server_id,secondary:server_id) = {
                    if ~proposed {
                        var msg : man_msg_t;
                        msg.view := view;
                        msg.lo := lo;
                        msg.hi := hi;
                        msg.primary := primary;
                        msg.secondary := secondary;
                        # Ask paxos to agree on our view change.
                        paxos.server.propose(client.manager.op_t.just(msg));
                        proposed := true;
                    }
                }

                # On a callback from Paxos, execute the view change.
                implement paxos.server.decide(inst:paxos.instance_t,op:op_t) {
                    proposed := false;
                    # Check that Paxos actually decided something.
                    if ~op.is_empty {
                        var msg := op.contents;
                        var conf : config;
                        conf.primary := msg.primary;
                        conf.secondary := msg.secondary;
                        var hi := msg.hi;
                        # Iterate over all shards in this range, and re-assign them.
                        while msg.lo < hi {
                            msg.hi := hi;
                            var lub := confmap.lub(msg.lo.next);
                            if lub < msg.hi {
                                msg.hi := lub;
                            }
                            assign_in_steps(msg);
                            confmap.set(msg.lo,msg.hi,conf);
                            msg.lo := msg.hi;
                        }
                    }
                }

                # Each view change is done in two steps to enable asynchrony:
                # 1. Make the new primary secondary.
                # 2. Make the secondary primary and replicate to new secondary.
                # The first step is only necessary if both the primary and secondary are changing.
                action assign_in_steps(msg:man_msg_t) = {
                    var old_config := confmap.get(msg.lo.val);
                    if old_config.primary ~= msg.primary & old_config.secondary ~= msg.primary {
                        var pmsg := msg;
                        pmsg.primary := old_config.primary;
                        pmsg.secondary := msg.primary;
                        broadcast(pmsg);
                    }
                    broadcast(msg);
                }

                # Broadcasts a new view message to all clients and servers.
                action broadcast(msg:man_msg_t) = {
                    view := view.next;
                    msg.view := view;
                    for it,cl in client_id.iter {
                        sock.send(client(cl).man_sock.id,msg);
                    }
                    for it,sv in server_id.iter {
                        sock.send(server(sv).man_sock.id,msg);
                    }
                }

                # Record time from server ping messages.
                implement sock.recv(src:tcp.endpoint,msg:man_msg_t) {
                    if msg.view = view {
                        heard(msg.src) := time;
                    }
                }

                # Paxos instantation.
                instance paxos : multi_paxos(manager_id,client.manager.op_t,client.manager.op_t.empty)

            } with client.manager.paxos.nset, manager.paxos.nset.api, manager.paxos.nset.majorities_intersect, client.manager.paxos.round_t, client.manager.paxos.instance_t, vector[client.manager.paxos.vote_t], client.manager.paxos, client.manager.paxos.server, nat

            process server(self:server_id) = {
                # Server network sockets.
                instance sock : net.socket
                instance man_sock : man_net.socket

                # Server state.


                implement sock.recv(src:tcp.endpoint,msg:msg_t) {
                    
                }

                implement man_sock.recv(src:tcp.endpoint,msg:man_msg_t) {
                    
                }
            }
        }

        #
        # Client implementation.
        #

        # Client state.
        #instance confmap : delegation_map(key_t,config)

        # Cliet network sockets.
        instance sock : net.socket
        instance man_sock : man_net.socket

        implement subscribe_req {
            
        }

        implement unsubscribe_req {

        }

        implement publish_req {

        }

        implement sock.recv(src:tcp.endpoint,msg:msg_t) {
            if msg.kind = sub_rsp_kind {
                subscribe_rsp(msg.key,msg.log);
            } else if msg.kind = unsub_rsp_kind {
                unsubscribe_rsp(msg.key);
            } else if msg.kind = pub_rsp_kind {
                publish_rsp(msg.key,msg.val);
            }
        }

        implement man_sock.recv(src:tcp.endpoint,msg:man_msg_t) {

        }
    }
}
