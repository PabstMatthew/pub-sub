#lang ivy1.8

# STL
include numbers
include network
include order
include collections

# Project-specific
include arrayset
include shard
include table
include delmap
include multi_paxos

# Types.
global {
    # IDs for different processes.
    instance client_id : iterable
    instance server_id : iterable
    instance manager_id : iterable

    # IDs that order publications.
    instance pub_id : iterable

    # Key type used to identify pub/sub channels.
    instance key_t : iterable

    # Value types used in channels.
    alias byte = uint[8]
    alias data_t = byte
    class val_t = {
        field data : data_t         # the actual publication request data
        field src : client_id       # which client requested this publication
        field idx : pub_id          # the total order index of this message in the channel
        field client_idx : pub_id   # the order of this message among this client's publications
                                    # (only used in ghost code)
    }
    instance log_t : vector(val_t)  # a history of channel messages
    instance client_set : arrayset(client_id)

    # Channel state.
    class channel_t = {
        field log : log_t                       # channel message history
        field clients_subscribed : client_set   # set of clients subscribed to this channel
        field pending_subs : client_set         # set of pending subscriptions
    }
}

process client (self:client_id) = {

    # Subscribe to a particular key.
    export action subscribe_req(key:key_t)
    # In response, receive `content`, a log of all previous messages for this key.
    # Once this message is received,the client will also receive `publish_rsp`
    # when values are published to this key.
    import action subscribe_rsp(key:key_t,content:log_t)

    # Unsubscribe from a subscribed channel.
    export action unsubscribe_req(key:key_t)
    # In response, receive a notification that no more publishes will be sent.
    import action unsubscribe_rsp(key:key_t)

    # Publish a value to this key. Expected to only be called after a corresponding 
    # `subscribe_req` and `subscribe_rsp`.
    export action publish_req(key:key_t,data:data_t)
    # Callback to notify clients when a value has been published to a key they've 
    # subscribed to. This includes values published by themself.
    import action publish_rsp(key:key_t,val:val_t)

    specification {

        common {

            var pending_subscribe(X:client_id,K:key_t) : bool           # a subscribe is pending for this key
            var pending_unsubscribe(X:client_id,K:key_t) : bool         # an unsubscribe is pending for this key
            var subscribed(X:client_id,K:key_t) : bool                  # this client is subscribed to this key
            var committed(X:client_id,K:key_t,C:pub_id) : bool          # has this client received a publication for this index?
            var commit_order(X:client_id,K:key_t,C:pub_id) : val_t      # the total publication order seen by each client to each key
            var pub_committed(X:client_id,K:key_t,C:pub_id) : bool      # has this publication been committed?
            var pub_rsps(X:client_id,K:key_t,C:pub_id) : val_t          # the publication commit order for this client

            # Checks if a commit is consistent with our requirements.
            function valid_commit(k:key_t,v:val_t) =
                                # The commit must agree with other clients' index for this key.
                                # Basically, everyone agrees on *some* order for the channel.
                                (forall X:client_id. committed(X,k,v.idx) -> commit_order(X,k,v.idx) = v) &
                                # The commit must have come from somewhere.
                                (exists P:pub_id. (pub_committed(v.src,k,P) & pub_rsps(v.src,k,P) = v))

            after init {
                pending_subscribe(X,K) := false;
                pending_unsubscribe(X,K) := false;
                subscribed(X,K) := false;
                committed(X,K,C) := false;
                pub_committed(X,K,C) := false;
            }

            before subscribe_req(self:client_id,key:key_t) {
                # Client must only have one pending subscribe per key, 
                # and must not currently be subscribed to this key.
                require ~pending_subscribe(self,key) & ~subscribed(self,key);
                pending_subscribe(self,key) := true;
            }

            before subscribe_rsp(self:client_id,key:key_t,content:log_t) {
                # Client must have requested this subscription.
                require pending_subscribe(self,key);
                pending_subscribe(self,key) := false;
                subscribed(self,key) := true;
                # Add all entries to this client's observed commit order, 
                # and check that they match other clients'.
                var i : index; i := 0;
                while i < content.end
                decreases content.end - i {
                    var val := content.get(i);
                    require valid_commit(key,val);
                    commit_order(self,key,val.idx) := val;
                    committed(self,key,val.idx) := true;
                    i := i.next;
                }
            }

            before unsubscribe_req(self:client_id,key:key_t) {
                # Client must only have one pending unsubcribe per key, 
                # and must currently be subscribed to this key.
                require ~pending_unsubscribe(self,key) & subscribed(self,key);
                pending_unsubscribe(self,key) := true;
            }

            before unsubscribe_rsp(self:client_id,key:key_t) {
                # Client must have requested this unsubscription.
                require pending_unsubscribe(self,key);
                pending_unsubscribe(self,key) := false;
                subscribed(self,key) := false;
            }

            before publish_req(self:client_id,key:key_t,data:data_t) {
                # Client must be subscribed and not be unsubscribing to publish.
                require subscribed(self,key) & ~pending_unsubscribe(self,key);
            }

            before publish_rsp(self:client_id,key:key_t,val:val_t) {
                # Client must still be subscribed to this key.
                require subscribed(self,key);
                # Keep track of this publication, and make sure it matches other clients.
                require valid_commit(key,val);
                commit_order(self,key,val.idx) := val;
                committed(self,key,val.idx) := true;
            }

            # Called when a client's publish has been "committed" server-side.
            # Here, I check that the publication is sequentially consistent.
            action commit(key:key_t,self:client_id,val:val_t) = {
                debug "commit" with key=key, client=self, val=val;
                # Sanity check that we only commit publictaions once.
                require ~pub_committed(self,key,val.client_idx);
                # If one of our publications is committed, we need to check that 
                # its total order obeys our private order (i.e. sequential consistency).
                require forall P:pub_id. ((P > val.client_idx & pub_committed(self,key,P)) -> (pub_rsps(self,key,P).idx > val.idx)) &
                                         ((P < val.client_idx & pub_committed(self,key,P)) -> (pub_rsps(self,key,P).idx < val.idx));
                pub_committed(self,key,val.client_idx) := true;
                pub_rsps(self,key,val.client_idx) := val;
            }

        }
    }

    implementation {

        common {

            # This type represents a range of keys and a list of key/value pairs in that range.
            global {
                instance shard_t : table_shard(key_t,channel_t)
            }

            # Describes a key's configuration.
            class config = {
                field primary : server_id   # the primary server for this key
                field secondary : server_id # the secondary server for this key
                field replicating : bool    # is this key being replicated? i.e. is it safe to serve requests?
            }

            # Clients and servers communicate using these message kinds:
            type msg_kind = {
                sub_req_kind,   # subscribe request
                sub_rsp_kind,   # subscribe response
                unsub_req_kind, # unsubscribe request
                unsub_rsp_kind, # unsubscribe response
                pub_req_kind,   # publish request
                pub_rsp_kind,   # publish response
                replicate_kind  # server-server log transfer
            }

            # Message class for client-server messages.
            class msg_t = {
                field kind : msg_kind           # kind of message
                field key : key_t               # key to operate on
                field val : val_t               # value, if any
                field log : log_t               # result of a subscribe request
                field src_client : client_id    # requesting client ID
                field primary : server_id       # who the primary is
                field shard : shard_t           # shard, only used for replicate messages
                field clients : client_set      # set of subscribed clients to send a message to, 
                                                # only used to maintain consistency when handling 
                                                # a publish request
                field secondary : server_id     # who the secondary is, only used for subscribe requests
            }

            # Network for client-server messages.
            instance net : tcp.net(msg_t)

            # Management messages have the following structure:
            class man_msg_t = {
                field view : nat            # view number (indicates configuration version)
                field lo : key_t.iter.t     # lower bound of key range
                field hi : key_t.iter.t     # upper bound of key range
                field primary : server_id   # primary server id, if any
                field secondary : server_id # secondary server id, if any
                field src : server_id       # pinging server id, if any
            }

            # Network for management messages.
            instance man_net : tcp.net(man_msg_t)

            process manager(self:manager_id) = {

                # Manager state.
                var view : nat                  # the current view number
                var proposed : bool             # have we proposed a new view?
                instance confmap : delegation_map(key_t,config)    # stores configuration for each key

                common {
                    # Paxos decides on this operation, but can optionally decide nothing.
                    instance op_t : option(man_msg_t)
                }

                # Manager's socket on management network.
                instance sock : man_net.socket

                after init {
                    view := 0;
                    proposed := false;
                    var conf : config;
                    conf.primary := 0;
                    conf.secondary := 1;
                    conf.replicating := false;
                    confmap.set(key_t.iter.begin,key_t.iter.end,conf);
                }

                # Managers take commands to assign a range of keys `[lo,hi)` to a given primary and secondary.
                export action assign(lokey:key_t,hikey:key_t,primary:server_id,secondary:server_id) = {
                    if primary ~= secondary {
                        announce(view+1, key_t.iter.create(lokey), key_t.iter.create(hikey), primary, secondary);
                    }
                }

                # Propose a view change to Paxos. The view change will only be broadcast once decided by Paxos.
                action announce(view:nat,lo:key_t.iter.t,hi:key_t.iter.t,primary:server_id,secondary:server_id) = {
                    if ~proposed {
                        var msg : man_msg_t;
                        msg.view := view;
                        msg.lo := lo;
                        msg.hi := hi;
                        msg.primary := primary;
                        msg.secondary := secondary;
                        # Ask paxos to agree on our view change.
                        paxos.server.propose(client.manager.op_t.just(msg));
                        proposed := true;
                    }
                }

                # On a callback from Paxos, execute the view change.
                implement paxos.server.decide(inst:paxos.instance_t,op:op_t) {
                    proposed := false;
                    # Check that Paxos actually decided something.
                    if ~op.is_empty {
                        var msg := op.contents;
                        var conf : config;
                        conf.primary := msg.primary;
                        conf.secondary := msg.secondary;
                        var hi := msg.hi;
                        # Iterate over all shards in this range, and re-assign them.
                        while msg.lo < hi {
                            msg.hi := hi;
                            var lub := confmap.lub(msg.lo.next);
                            if lub < msg.hi {
                                msg.hi := lub;
                            }
                            assign_in_steps(msg);
                            confmap.set(msg.lo,msg.hi,conf);
                            msg.lo := msg.hi;
                        }
                    }
                }

                # Each view change is done in two steps to enable asynchrony:
                # 1. Make the new primary secondary.
                # 2. Make the secondary primary and replicate to new secondary.
                # The first step is only necessary if both the primary and secondary are changing.
                action assign_in_steps(msg:man_msg_t) = {
                    var old_config := confmap.get(msg.lo.val);
                    if old_config.primary ~= msg.primary & old_config.secondary ~= msg.primary {
                        var pmsg := msg;
                        pmsg.primary := old_config.primary;
                        pmsg.secondary := msg.primary;
                        broadcast(pmsg);
                    }
                    broadcast(msg);
                }

                # Broadcasts a new view message to all clients and servers.
                action broadcast(msg:man_msg_t) = {
                    view := view.next;
                    msg.view := view;
                    for it,cl in client_id.iter {
                        sock.send(client(cl).man_sock.id,msg);
                    }
                    for it,sv in server_id.iter {
                        sock.send(server(sv).man_sock.id,msg);
                    }
                }

                # Paxos instantation.
                instance paxos : multi_paxos(manager_id,client.manager.op_t,client.manager.op_t.empty)

            } with client.manager.paxos.nset, manager.paxos.nset.api, manager.paxos.nset.majorities_intersect, client.manager.paxos.round_t, client.manager.paxos.instance_t, vector[client.manager.paxos.vote_t], client.manager.paxos, client.manager.paxos.server, nat

            process server(self:server_id) = {

                # Server network sockets.
                instance sock : net.socket
                instance man_sock : man_net.socket

                # Server state.
                instance table : hash_table(key_t,channel_t,shard_t)    # K/V replica
                var view : nat                                          # view version
                instance confmap : delegation_map(key_t,config)         # configuration
                instance view_queue : unbounded_queue(man_msg_t)        # holds unprocessed view changes
                var replicating : bool                                  # is this server replicating new keys now?
                var last_pub(K:key_t) : index                           # the number of publications completed (and clients notified) for a given key
                var max_client_idx(K:key_t) : pub_id                    # the largest client_idx we've seen for this key

                after init {
                    var conf : config;
                    conf.primary := 0;
                    conf.secondary := 1;
                    conf.replicating := false;
                    confmap.set(key_t.iter.begin,key_t.iter.end,conf);
                    view := 0;
                    replicating := false;
                }

                # Client-server network.
                implement sock.recv(src:tcp.endpoint,msg:msg_t) {
                    if msg.kind = sub_req_kind {
                        # The interaction between subscribes and publishes is extremely tricky. 
                        # I need to ensure that no pub_rsp are sent to clients before sub_rsp, 
                        # but also that no messages are lost in this process, and that all clients 
                        # see the same order of messages in each channel.

                        # To avoid any races, I use the following protocol:
                        # 1. The primary first sends the request to the secondary
                        # 2. The intended secondary records the request in pending_subs, but does not persistently complete it, 
                        #    and sends a response to the whomever it thinks is the primary (may not be the original primary).
                        # 3. If the recipient of that message is not the primary, it sends it to whomever it thinks the new primary is, repeating the process.
                        #    If the recipient of that message is the primary and the secondary has changed, the request is restarted.
                        #    If the recipient of that message is the primary and the secondary hasn't changed, it updates it state, and notifies the client and the secondary.
                        # Additionally, if a secondary gets promoted, its pending_subs are replicated with the shard.

                        # Here's a quick argument that this strategy works by looking at the tricky cases:
                        # - If the primary gets reassigned before the secondary responds, the following cases could occur:
                        #   - the secondary receives the message before it is promoted to primary, and notifies the primary, who will restart the request.
                        #   - the secondary changes to primary, then to something else before the message arrives, meaning it will restart the request.
                        #   - the secondary changes to primary, receives the request, then restarts the request.
                        # - If the secondary gets reassigned before a response from the original secondary, 
                        #   the primary will notice, and restart the request.
                        # Basically, if the primary and secondary are not reassigned for 3 message exchanges, the request will succeed,
                        # and otherwise, it will be restarted.
                        
                        # While this process is happening, publishes to this channel may occur, and as a result,
                        # each sub_rsp may need these intervening messages added. This is accomplished by simply 
                        # tracking the number of completed publishes and copying any extras to subscribe responses.

                        var conf := confmap.get(msg.key);
                        if conf.primary = self {
                            #debug "primary notifying secondary" with primary=self, secondary=conf.secondary, key=msg.key, client=msg.src_client;
                            # Notify the secondary, and wait for a response.
                            msg.secondary := conf.secondary;
                            sock.send(server(conf.secondary).sock.id,msg);
                        } else if msg.secondary = self {
                            # Note that we are msg.secondary, and not necessarily conf.secondary.
                            # This is an important distinction because multiple servers may think they are conf.secondary 
                            # for a particular key, which could cause duplicate subscribe responses. Only the intended secondary 
                            # recipient should respond to avoid this problem.

                            #debug "msg.secondary notifying primary" with primary=conf.primary, conf_secondary=conf.secondary, msg_secondary=self, key=msg.key, client=msg.src_client;
                            # Record the subscribe, and respond to the primary.
                            msg.kind := sub_rsp_kind;
                            var chan := table.get(msg.key);
                            chan.pending_subs := chan.pending_subs.add(msg.src_client);
                            table.set(msg.key,chan);
                            msg.log := chan.log;
                            sock.send(server(conf.primary).sock.id,msg);
                        }
                    } else if msg.kind = sub_rsp_kind {
                        var conf := confmap.get(msg.key);
                        if msg.secondary = self { 
                            # This message was sent by a primary to notify us of a completed subscribe.
                            #debug "secondary recv'd ACK" with self=self, primary=conf.primary, secondary=conf.secondary, key=msg.key, client=msg.src_client;
                            if conf.secondary = self | conf.primary = self {
                                # If we're the secondary, the primary ACK'd this subscribe, so we're safe to complete it.
                                # If we're the primary, we must have been promoted before receiving the ACK, so we need to complete the operation.
                                var chan := table.get(msg.key);
                                chan.pending_subs := chan.pending_subs.remove(msg.src_client);
                                chan.clients_subscribed := chan.clients_subscribed.add(msg.src_client);
                                table.set(msg.key,chan);
                            } else {
                                # I'm no longer the secondary, in which case, the new secondary should have inherited 
                                # the subscription from the primary, and we're all clear to drop this ACK.
                            }
                        } else if conf.primary = self { 
                            # I am the original primary.
                            if conf.secondary ~= msg.secondary {
                                # The secondary has changed, so we need to retry the operation.
                                # If the new secondary receives this as the new secondary, it will complete it normally.
                                # Else, the new secondary receives this as the new primary and will retry.
                                #debug "still primary, retrying" with primary=self, secondary=conf.secondary, key=msg.key, client=msg.src_client;
                                msg.secondary := conf.secondary;
                                msg.kind := sub_req_kind;
                                sock.send(server(conf.secondary).sock.id,msg);
                            } else {
                                # The secondary received the subscribe, so we're safe to complete.
                                #debug "primary received response from secondary, completing" with primary=self, secondary=conf.secondary, key=msg.key, client=msg.src_client;
                                var chan := table.get(msg.key);
                                chan.clients_subscribed := chan.clients_subscribed.add(msg.src_client);
                                chan.pending_subs := chan.pending_subs.remove(msg.src_client);
                                table.set(msg.key,chan);
                                # Catch the client up on any new messages.
                                var idx : index; idx := 0;
                                var log_size := msg.log.size();
                                while idx < last_pub(msg.key) {
                                    if idx >= log_size {
                                        msg.log := msg.log.append(chan.log.get(idx));
                                    }
                                    idx := idx.next;
                                }
                                sock.send(client(msg.src_client).sock.id,msg);
                                # Notify the secondary of success.
                                msg.secondary := conf.secondary;
                                sock.send(server(conf.secondary).sock.id,msg);
                            }
                        } else {
                            #debug "got reassigned, notifying primary to retry" with primary=conf.primary, key=msg.key, client=msg.src_client;
                            # I'm no longer the primary, so I need to tell the new primary to retry.
                            msg.kind := sub_req_kind;
                            sock.send(server(conf.primary).sock.id,msg);
                        }
                    } else if msg.kind = unsub_req_kind {
                        var conf := confmap.get(msg.key);
                        if conf.primary = self {
                            # Here is the plan: we remove this client from our set to be conservative, 
                            # alert the secondary, but wait for secondary to complete before responding.
                            var chan := table.get(msg.key);
                            chan.clients_subscribed := chan.clients_subscribed.remove(msg.src_client);
                            table.set(msg.key,chan);
                            msg.secondary := conf.secondary;
                            sock.send(server(conf.secondary).sock.id,msg);
                        } else if msg.secondary = self {
                            if conf.secondary ~= self {
                                # I've been re-assigned, so I need to make sure the request survives.
                                sock.send(server(conf.primary).sock.id,msg);
                            } else {
                                # I think I'm still the secondary, so I should complete the request, and notify the client.
                                # Here's a quick justification:
                                #   If I become primary, I won't send publishes to this client.
                                #   If some other server becomes the secondary, it should have replicated the primary's subscriber list.
                                var chan := table.get(msg.key);
                                chan.clients_subscribed := chan.clients_subscribed.remove(msg.src_client);
                                table.set(msg.key,chan);
                                msg.kind := unsub_rsp_kind;
                                # Send response to the primary, which ensures that the client response 
                                # happens after all in-flight publishes complete.
                                sock.send(server(conf.primary).sock.id,msg);
                            }
                        }
                    } else if msg.kind = unsub_rsp_kind {
                        # Forward the response to the client.
                        sock.send(client(msg.src_client).sock.id,msg);
                    } else if msg.kind = pub_req_kind {
                        # Here's a race condition:
                        # The primary accepts a publication request, and forwards to secondary.
                        # Before the request arrives at the secondary, the secondary gets promoted and starts handling new pubs.
                        # Now, when the request arrives at the new primary, it may be committed after newer requests. 
                        # Solution: primary will only handle pub reqs if their client_idx is greater than the max for this client.
                        var conf := confmap.get(msg.key);
                        var chan := table.get(msg.key);
                        if conf.primary = self {
                            # Only allow publications if we've dealt with all pending subscriptions from previous views.
                            if chan.pending_subs.size = 0 {
                                # Prevent zombie requests from earlier views from breaking sequential consistency.
                                if msg.val.client_idx > max_client_idx(msg.key) {
                                    max_client_idx(msg.key) := msg.val.client_idx;
                                    if chan.log.size = 0 {
                                        msg.val.idx := 0;
                                    } else {
                                        msg.val.idx := chan.log.get(chan.log.size-1).idx.next;
                                    }
                                    chan.log := chan.log.append(msg.val);
                                    table.set(msg.key,chan);
                                    sock.send(server(conf.secondary).sock.id,msg);
                                    # Receiving this request has serialized its order.
                                    serialize(msg.key,self,conf.secondary,msg.src_client,msg.val);
                                }
                            }
                        } else if conf.secondary = self & ~conf.replicating{
                            if src = server(conf.primary).sock.id {
                                if chan.log.size = 0 {
                                    msg.val.idx := 0;
                                } else {
                                    msg.val.idx := chan.log.get(chan.log.size-1).idx.next;
                                }
                                chan.log := chan.log.append(msg.val);
                                table.set(msg.key,chan);
                                msg.log := chan.log; # only used for tracking purposes, not actually used by clients.
                                msg.kind := pub_rsp_kind;
                                sock.send(server(conf.primary).sock.id,msg);
                                # This publish is now effectively "committed".
                                commit_one(msg.key,msg.src_client);
                            }
                        }
                    } else if msg.kind = pub_rsp_kind {
                        # Broadcast the publish to all subscribed clients.
                        var chan := table.get(msg.key);
                        for it,cl in client_id.iter {
                            if chan.clients_subscribed.contains(cl) {
                                sock.send(client(cl).sock.id,msg);
                            }
                        }
                        # Update state.
                        last_pub(msg.key) := msg.log.size();
                    } else if msg.kind = replicate_kind {
                        # A new primary has sent us their data to replicate it.
                        var conf := confmap.get(msg.key);
                        table.incorporate(msg.shard);
                        conf.secondary := self;
                        conf.primary := msg.primary;
                        conf.replicating := false;
                        confmap.set(msg.shard.lo,msg.shard.hi,conf);
                        replicating := false;
                        # Process any view changes we ignored while waiting to replicate.
                        while ~view_queue.empty & ~replicating {
                            process_view(view_queue.dequeue);
                        }
                    }
                }

                action process_view(msg:man_msg_t) = {
                    # Ignore duplicates resulting from multi-Paxos.
                    if msg.view = view+1 {
                        var conf := confmap.get(msg.lo.val);
                        if msg.primary = self & conf.secondary ~= msg.secondary {
                            # We are the new primary.
                            debug "becoming primary" with server=self, lo=msg.lo.val, hi=msg.hi.val;
                            # Send the new secondary our data.
                            var rmsg : msg_t;
                            rmsg.kind := replicate_kind;
                            rmsg.shard := table.extract_(msg.lo,msg.hi);
                            rmsg.primary := self;
                            sock.send(server(msg.secondary).sock.id,rmsg);
                            # Call the ghost action to commit any outstanding pubs for this key.
                            var it := msg.lo;
                            while it < msg.hi {
                                commit_all_serialized(it.val,self,conf.secondary);
                                it := it.next;
                            }
                        }
                        if self = msg.secondary & self ~= conf.secondary {
                            # If we are the new secondary, wait for a replicate message.
                            debug "becoming secondary" with server=self, lo=msg.lo.val, hi=msg.hi.val;
                            conf.replicating := true;
                            replicating := true;
                        }
                        # Update our state.
                        view := msg.view;
                        conf.primary := msg.primary;
                        conf.secondary := msg.secondary;
                        confmap.set(msg.lo,msg.hi,conf);
                    }
                }

                # Management message informing us of view change.
                implement man_sock.recv(src:tcp.endpoint,msg:man_msg_t) {
                    if replicating {
                        # We're in the middle of another replicate operation, so hold off on this one.
                        view_queue.enqueue(msg);
                    } else {
                        # Otherwise, we can handle the new view.
                        process_view(msg);
                    }
                }

                # Ghost code to keep track of publish orders (per key and client)
                # to check sequential consistency.
                specification {
                    # Keeps track of publish requests serialized at the primary, 
                    # but not committed by the secondary.
                    instance serialized(K:key_t,C:client_id) : unbounded_queue(val_t)

                    # Called by the secondary to commit a publish.
                    action commit_one(k:key_t,src_client:client_id) = {
                        debug "commit_one" with key=k, secondary=self;
                        if ~serialized(k,src_client).empty {
                            commit(k,src_client,serialized(k,src_client).dequeue);
                        }
                    }

                    common {
                        var true_primary(K:key_t) : server_id

                        # Initially, server 0 is the primary for all keys.
                        after init {
                            true_primary(K) := 0;
                        }

                        # This request has arrived at the primary, meaning its relative order is fixed/serialized.
                        action serialize(k:key_t,primary:server_id,secondary:server_id,src_client:client_id,val:val_t) = {
                            debug "serialize" with key=k, primary=primary, secondary=secondary, val=val;
                            if primary = true_primary(k) {
                                serialized(secondary,k,src_client).enqueue(val);
                            }
                        }

                        # After replicating, this ghost action is called to dequeue outstanding publishes.
                        action commit_all_serialized(k:key_t,self:server_id,secondary:server_id) = {
                            debug "commit_all_serialized" with key=k, primary=self, secondary=secondary;
                            for it,src_client in client_id.iter {
                                while ~serialized(secondary,k,src_client).empty 
                                decreases serialized(secondary,k,src_client).tail - serialized(secondary,k,src_client).head {
                                    var val := serialized(secondary,k,src_client).dequeue;
                                    if self = true_primary(k) {
                                        commit(k,src_client,val);
                                    }
                                }
                            }
                            true_primary(k) := self;
                        }
                    }
                }
            }
        }

        #
        # Client implementation.
        #

        # Client state.
        instance confmap : delegation_map(key_t,config)
        var private_pub_num(K:key_t) : pub_id           # the number of publication requests for this client

        # Cliet network sockets.
        instance sock : net.socket
        instance man_sock : man_net.socket

        after init {
            var conf : config;
            conf.primary := 0;
            conf.secondary := 1;
            conf.replicating := false;
            confmap.set(key_t.iter.begin,key_t.iter.end,conf);
        }

        implement subscribe_req {
            var msg : msg_t;
            msg.kind := sub_req_kind;
            msg.src_client := self;
            msg.key := key;
            var conf := confmap.get(key);
            var primary := conf.primary;
            sock.send(server(primary).sock.id,msg);
        }

        implement unsubscribe_req {
            var msg : msg_t;
            msg.kind := unsub_req_kind;
            msg.src_client := self;
            msg.key := key;
            var conf := confmap.get(key);
            var primary := conf.primary;
            sock.send(server(primary).sock.id,msg);
        }

        implement publish_req {
            var val : val_t;
            val.data := data;
            val.src := self;
            val.client_idx := private_pub_num(key);
            private_pub_num(key) := private_pub_num(key).next;
            var msg : msg_t;
            msg.kind := pub_req_kind;
            msg.src_client := self;
            msg.key := key;
            msg.val := val;
            var conf := confmap.get(key);
            var primary := conf.primary;
            sock.send(server(primary).sock.id,msg);
        }

        implement sock.recv(src:tcp.endpoint,msg:msg_t) {
            if msg.kind = sub_rsp_kind {
                subscribe_rsp(msg.key,msg.log);
            } else if msg.kind = unsub_rsp_kind {
                unsubscribe_rsp(msg.key);
            } else if msg.kind = pub_rsp_kind {
                publish_rsp(msg.key,msg.val);
            }
        }

        implement man_sock.recv(src:tcp.endpoint,msg:man_msg_t) {
            var conf : config;
            conf.primary := msg.primary;
            conf.secondary := msg.secondary;
            confmap.set(msg.lo,msg.hi,conf);
        }
    }
}
